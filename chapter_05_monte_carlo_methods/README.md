# **Chapter 5: Monte Carlo Methods** &nbsp; &nbsp; :link: [Notes](./%5BNOTES%5DCH_5.pdf)

## Examples

### 5.1 Blackjack (*p.94*)

Approximate state-value functions for the blackjack policy that sticks only on 20
or 21, computed by Monte Carlo policy evaluation. [Code](./example_5_1_blackjack.py)
<br>

**Usable ace:**

<p align="center">
    <img src='./plots/example_5_1/10000_episodes_usable_ace.png' width=45%>
    <img src='./plots/example_5_1/500000_episodes_usable_ace.png' width=45%>
</p>

**No usable ace:**

<p align="center">
    <img src='./plots/example_5_1/10000_episodes_no_usable_ace.png' width=45%>
    <img src='./plots/example_5_1/500000_episodes_no_usable_ace.png' width=45%>
</p>

### 5.3 Solving Blackjack (*p.99*)
Apply Monte Carlo ES to blackjack. The initial policy is to stick only on the player's sum is 20 or 21, and the initial action-value function is zero for all state-action pairs. [Code](./example_5_3_solving_blackjack.py)
<br>

**Usable ace:**

<p align="center">
    <img src='./plots/example_5_3/optimal_policy_usable_ace.png' width=45%>
    <img src='./plots/example_5_3/optimal_value_usable_ace.png' width=40.25%>
</p>

**No usable ace:**

<p align="center">
    <img src='./plots/example_5_3/optimal_policy_no_usable_ace.png' width=45%>
    <img src='./plots/example_5_3/optimal_value_no_usable_ace.png' width=40.25%>
</p>

### 5.4 Off-policy Estimation of a Blackjack State Value (*p.106*)
Evaluate the state in which the dealer is showing a deuce, the sum of the player’s cards is 13, and the player has a usable ace. The data was generated by starting in this state then choosing to hit or stick at random with equal probability (the behavior policy). The target policy was to stick only on a sum of 20 or 21, as in Example 5.1. The value of this state under the target policy is approximately −0.27726. [Code](./example_5_4_off_policy_estimation.py)
<br>

<p align="center">
    <img src='./plots/example_5_4.png' width=80%>
</p>


### 5.5 Infinite Variance (*p.107*)
Ordinary importance sampling produces surprisingly unstable estimates on the one-state MDP shown inset. The correct estimate here is 1 ($\gamma$ = 1), and, even though this is the expected value of a sample return (after importance sampling), the variance of the samples is infinite, and the estimates do not converge to this value. These results are for on-policy every-visit MC. [Code](./example_5_5_infinite_var.py)

<p align="center">
    <img src='./plots/example_5_5.png' width=80%>
</p>